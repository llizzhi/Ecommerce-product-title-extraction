{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Zhijun\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.267 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "D:\\abc\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import jieba.analyse\n",
    "from helper import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator\n",
    "jieba.initialize() \n",
    "#jieba.set_dictionary(\"dict.txt.big\")\n",
    "jieba.load_userdict(\"user_dict.txt\")\n",
    "#jieba.analyse.set_stop_words(\"stopwords.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', encoding = 'utf-8')\n",
    "test = pd.read_csv('test.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.1 Remove anomalies in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train[(train['Product Name'] != '#NAME?') & (train['Product Name'] != '#ERROR!')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_impression = train.groupby(['Product Name', 'Category','Query']).size().reset_index()\n",
    "num_of_impression.columns.values[3] = 'num_of_impressions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_click = train[train.Event == 'Click'].groupby(['Product Name', 'Category','Query']).size().reset_index()\n",
    "num_of_click.columns.values[3] = 'num_of_clicks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Query</th>\n",
       "      <th>Event</th>\n",
       "      <th>Date</th>\n",
       "      <th>num_of_impressions</th>\n",
       "      <th>num_of_clicks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--- X 10 --- 七色 多層次搭配 圓下擺 LAYERED 素面 無袖背心 打底</td>\n",
       "      <td>Male Fashion</td>\n",
       "      <td>無袖</td>\n",
       "      <td>Impression</td>\n",
       "      <td>31/7/17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>︱IBIT︱Gymshark 熱銷款 運動T恤 健身T恤 圓領短T 運動短T 健身鯊魚</td>\n",
       "      <td>Male Fashion</td>\n",
       "      <td>gymshark</td>\n",
       "      <td>Impression</td>\n",
       "      <td>31/7/17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>︱IBIT︱Gymshark 超高彈性 短褲 運動短褲 跑步短褲 深蹲褲 訓練短褲</td>\n",
       "      <td>Male Fashion</td>\n",
       "      <td>gymshark</td>\n",
       "      <td>Impression</td>\n",
       "      <td>31/7/17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>::另類情侶兄弟姊妹殼::電力滿格/不足黑白趣味浮雕手機軟殼i5/i5s/i5se/i6/i...</td>\n",
       "      <td>Mobile &amp; Gadgets</td>\n",
       "      <td>軟殼</td>\n",
       "      <td>Click</td>\n",
       "      <td>31/7/17</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>：新舊手機商場：Iphone6 16金 （需要看細圖密我）</td>\n",
       "      <td>Mobile &amp; Gadgets</td>\n",
       "      <td>iphone6 系列</td>\n",
       "      <td>Impression</td>\n",
       "      <td>30/7/17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name          Category  \\\n",
       "0       --- X 10 --- 七色 多層次搭配 圓下擺 LAYERED 素面 無袖背心 打底      Male Fashion   \n",
       "1        ︱IBIT︱Gymshark 熱銷款 運動T恤 健身T恤 圓領短T 運動短T 健身鯊魚      Male Fashion   \n",
       "2          ︱IBIT︱Gymshark 超高彈性 短褲 運動短褲 跑步短褲 深蹲褲 訓練短褲      Male Fashion   \n",
       "3  ::另類情侶兄弟姊妹殼::電力滿格/不足黑白趣味浮雕手機軟殼i5/i5s/i5se/i6/i...  Mobile & Gadgets   \n",
       "4                      ：新舊手機商場：Iphone6 16金 （需要看細圖密我）  Mobile & Gadgets   \n",
       "\n",
       "        Query       Event     Date  num_of_impressions  num_of_clicks  \n",
       "0          無袖  Impression  31/7/17                   1            0.0  \n",
       "1    gymshark  Impression  31/7/17                   1            0.0  \n",
       "2    gymshark  Impression  31/7/17                   1            0.0  \n",
       "3          軟殼       Click  31/7/17                   1            1.0  \n",
       "4  iphone6 系列  Impression  30/7/17                   1            0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.merge(num_of_impression, how = 'left', on = ['Product Name', 'Category','Query'])\n",
    "train = train.merge(num_of_click, how = 'left', on = ['Product Name', 'Category','Query'])\n",
    "train.fillna(0, inplace = True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['click_per_impre'] = train['num_of_clicks'] / train['num_of_impressions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Split dataset by product categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4169, 9)\n",
      "(3064, 9)\n",
      "(3070, 9)\n"
     ]
    }
   ],
   "source": [
    "mg_train = train[train.Category == 'Mobile & Gadgets'].reset_index()\n",
    "ff_train = train[train.Category == 'Female Fastion'].reset_index()\n",
    "mf_train = train[train.Category == 'Male Fashion'].reset_index()\n",
    "print(mg_train.shape)\n",
    "print(ff_train.shape)\n",
    "print(mf_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Clean text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword_list = stopwordslist('stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disambiguation_mg(text):\n",
    "    \n",
    "    text = re.sub('i phone', 'iphone', text.lower())\n",
    "\n",
    "    \n",
    "    # plus, prime, edge (eg: \"j7 prime\" --> \"j7prime\")\n",
    "    for word in ['plus', 'edge', 'prime']:\n",
    "        pattern = '[0-9a-z三星samsung]+ ' + word\n",
    "        old_parts = re.findall(pattern, text)\n",
    "        new_parts = [re.sub(' ' + word, word, part) for part in old_parts]\n",
    "    \n",
    "        for i in range(len(old_parts)):\n",
    "            text = re.sub(old_parts[i], new_parts[i], text)\n",
    "            \n",
    "    # iphone (eg: \"iphone 6s\" --> \"iphone6s\")\n",
    "    if ('iphone' in text):\n",
    "        old_parts = re.findall('iphone [0-9seplus]+', text)\n",
    "        new_parts = [re.sub('iphone ', 'iphone', part) for part in old_parts]\n",
    "        \n",
    "        for i in range(len(old_parts)):\n",
    "            text = re.sub(old_parts[i], new_parts[i], text)  \n",
    "    \n",
    "    # ipad\n",
    "    if ('ipad' in text):\n",
    "        old_parts = re.findall('ipad [0-9miniproair]+', text)\n",
    "        new_parts = [re.sub('ipad ', 'ipad', part) for part in old_parts]\n",
    "        \n",
    "        for i in range(len(old_parts)):\n",
    "            text = re.sub(old_parts[i], new_parts[i], text)  \n",
    "            \n",
    "    # ipod\n",
    "    if ('ipod' in text):\n",
    "        old_parts = re.findall('ipod [touchnano]+', text)\n",
    "        new_parts = [re.sub('ipod ', 'ipod', part) for part in old_parts]\n",
    "        \n",
    "        for i in range(len(old_parts)):\n",
    "            text = re.sub(old_parts[i], new_parts[i], text) \n",
    "            \n",
    "    # 'note 4/5/6', or 'pro 4/5/6'\n",
    "    for word in ['note', 'pro']:\n",
    "        if (word in text):\n",
    "            pattern = word + ' [0-9]+'\n",
    "            old_parts = re.findall(pattern, text)\n",
    "            new_parts = [re.sub(word + ' ', word, part) for part in old_parts]\n",
    "        \n",
    "            for i in range(len(old_parts)):\n",
    "                text = re.sub(old_parts[i], new_parts[i], text) \n",
    "            \n",
    "    # unit word\n",
    "    for word in ['角', '入','万', '天','号', '代', '年','元', '折', 'mah', 'cm', '吋', '寸','毫安', '公分', '色']:\n",
    "        if (word in text):\n",
    "            pattern = '[0-9.]+ ' + word\n",
    "            old_parts = re.findall(pattern, text)\n",
    "            new_parts = [re.sub(' '+ word, word, part) for part in old_parts]\n",
    "    \n",
    "            for i in range(len(old_parts)):\n",
    "                text = re.sub(old_parts[i], new_parts[i], text)    \n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_processing(text, mode, cate_id, stopwords = [' '] + stopword_list):\n",
    "    \n",
    "    # replace emoji\n",
    "    try:\n",
    "        # UCS-4\n",
    "        highpoints = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])|([\\U000025A0-\\U000025FF])|([\\U00002500-\\U0000257F])|([\\U00002B50])|([\\U000010E6])|(\\U0000F8FF)')\n",
    "    except re.error:\n",
    "        # UCS-2\n",
    "        highpoints = re.compile(u'([\\u2600-\\u27BF])|([\\uD83C][\\uDF00-\\uDFFF])|([\\uD83D][\\uDC00-\\uDE4F])|([\\uD83D][\\uDE80-\\uDEFF])|([\\u25A0-\\u25FF])|([\\u2500-\\u257F])|([\\u2B50])|([\\u10e6])|(\\uf8ff)')\n",
    "    \n",
    "    res = highpoints.sub(u'??', text)  \n",
    "    \n",
    "    # for english character: convert to lower case\n",
    "    res = res.lower()\n",
    "    \n",
    "    if cate_id == 'mg':\n",
    "        res = disambiguation_mg(res)\n",
    "    elif cate_id == 'ff':\n",
    "        res = res\n",
    "    elif cate_id == 'mf':\n",
    "        res = res\n",
    "    \n",
    "    # split words\n",
    "    word_list = jieba.lcut_for_search(res, HMM=True)\n",
    "    \n",
    "    # remove punctuation & stopwords\n",
    "    if mode == 'simplified':\n",
    "        word_list = [Converter('zh-hans').convert(ele) for ele in word_list if ele not in stopwords]\n",
    "    elif mode == 'traditional':\n",
    "        word_list = [ele for ele in word_list if ele not in stopwords]\n",
    "    \n",
    "        \n",
    "    # remove substring\n",
    "    delete_words = []\n",
    "    for idx in range(len(word_list)):\n",
    "        for i in range((idx+1), len(word_list)):\n",
    "            if ((word_list[idx] in word_list[i]) & (word_list[idx] != word_list[i])):\n",
    "                delete_words.append(word_list[idx]) \n",
    "    word_list = [word for word in word_list if word not in delete_words]    \n",
    "        \n",
    "    # combine words as string \n",
    "    if len(word_list) < 1:\n",
    "        word_list = text.strip()\n",
    "    else:\n",
    "        text = ' '.join(word_list)\n",
    "        \n",
    "    if cate_id == 'mg':\n",
    "        text = disambiguation_mg(text)\n",
    "    elif cate_id == 'ff':\n",
    "        text = text\n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prep(df, cate_id):\n",
    "    \n",
    "    stopword_list = stopwordslist('stopwords.txt')\n",
    "    \n",
    "    df['Product_Name_s'] = df.apply(lambda row: pre_processing(row['Product Name'], mode = 'simplified', cate_id = cate_id), axis = 1)\n",
    "    df['Product_Name_t'] = df.apply(lambda row: pre_processing(row['Product Name'], mode = 'traditional', cate_id = cate_id), axis = 1)\n",
    "    df['Query_s'] = df.apply(lambda row: pre_processing(row['Query'], mode = 'simplified', cate_id = cate_id), axis = 1)\n",
    "    df['Query_t'] = df.apply(lambda row: pre_processing(row['Query'], mode = 'traditional', cate_id = cate_id), axis = 1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mg_train = data_prep(mg_train, 'mg')\n",
    "ff_train = data_prep(ff_train, 'ff')\n",
    "mf_train = data_prep(mf_train, 'mf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mg_train.to_csv('mg_train.csv', index = False, encoding = 'utf_8_sig')\n",
    "ff_train.to_csv('ff_train.csv', index = False, encoding = 'utf_8_sig')\n",
    "mf_train.to_csv('mf_train.csv', index = False, encoding = 'utf_8_sig')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
